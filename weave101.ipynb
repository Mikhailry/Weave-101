{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import weave\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmikhailry\u001b[0m (\u001b[33mmikhailry-paylocity\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure():\n",
    "    load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration\n",
    "configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weave will track the inputs, outputs and code of this function\n",
    "@weave.op()\n",
    "def extract_dinos(sentence: str) -> dict:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"In JSON format extract a list of `dinosaurs`, with their `name`, \n",
    "their `common_name`, and whether its `diet` is a herbivore or carnivore\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sentence\n",
    "            }\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: mikhailry.\n",
      "View Weave data at https://wandb.ai/mikhailry-paylocity/jurassic-park/weave\n",
      "游꼴 https://wandb.ai/mikhailry-paylocity/jurassic-park/r/call/0192caad-171a-7f22-9dda-454190f5b102\n",
      "{\n",
      "  \"dinosaurs\": [\n",
      "    {\n",
      "      \"name\": \"Tyrannosaurus rex\",\n",
      "      \"common_name\": \"T. rex\",\n",
      "      \"diet\": \"carnivore\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Triceratops\",\n",
      "      \"common_name\": \"Trike\",\n",
      "      \"diet\": \"herbivore\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Brachiosaurus\",\n",
      "      \"common_name\": \"Brachi\",\n",
      "      \"diet\": \"herbivore\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Initialise the weave project\n",
    "weave.init('jurassic-park')\n",
    "\n",
    "sentence = \"\"\"I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), \\\n",
    "both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant \\\n",
    "Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below.\"\"\"\n",
    "\n",
    "result = extract_dinos(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/mikhailry-paylocity/jurassic-park/r/call/0192cb4b-387d-75b0-be62-06c10c39da07\n",
      "{'n_dinosaurs': 3, 'dinosaurs': {'dinosaurs': [{'name': 'Tyrannosaurus rex', 'common_name': 'T. rex', 'diet': 'carnivore'}, {'name': 'Triceratops', 'common_name': 'Trike', 'diet': 'herbivore'}, {'name': 'Brachiosaurus', 'common_name': 'Brachi', 'diet': 'herbivore'}]}}\n"
     ]
    }
   ],
   "source": [
    "@weave.op()\n",
    "def extract_dinos(sentence: str) -> dict:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Extract any dinorsaur `name`, their `common_name`, \\\n",
    "  names and whether its `diet` is a herbivore or carnivore, in JSON format.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sentence\n",
    "            }\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@weave.op()\n",
    "def count_dinos(dino_data: dict) -> int:\n",
    "    # count the number of items in the returned list\n",
    "    k = list(dino_data.keys())[0]\n",
    "    return len(dino_data[k])\n",
    "\n",
    "@weave.op()\n",
    "def dino_tracker(sentence: str) -> dict:\n",
    "    # extract dinosaurs using a LLM\n",
    "    dino_data = extract_dinos(sentence)\n",
    "    \n",
    "    # count the number of dinosaurs returned\n",
    "    dino_data = json.loads(dino_data)\n",
    "    n_dinos = count_dinos(dino_data)\n",
    "    return {\"n_dinosaurs\": n_dinos, \"dinosaurs\": dino_data}\n",
    "\n",
    "weave.init('jurassic-park')\n",
    "\n",
    "sentence = \"\"\"I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), \\\n",
    "both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant \\\n",
    "Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below.\"\"\"\n",
    "\n",
    "result = dino_tracker(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/mikhailry-paylocity/jurassic-park/r/call/0192cb58-a2d8-7ac0-89bd-7e078602f048\n"
     ]
    }
   ],
   "source": [
    "import weave \n",
    "\n",
    "weave.init('jurassic-park')\n",
    "\n",
    "sentence = \"\"\"I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), \\\n",
    "both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant \\\n",
    "Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below.\"\"\"\n",
    "\n",
    "# track metadata alongside our previously defined function\n",
    "with weave.attributes({'user_id': 'lukas', 'env': 'production'}):\n",
    "    result = dino_tracker(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models\n",
    "A Model is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave import Model\n",
    "import weave\n",
    "\n",
    "class RainyLLM(Model):\n",
    "    attribute1: str\n",
    "    attribute2: int\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, input_data: str) -> dict:\n",
    "        # Model logic goes here\n",
    "        prediction = self.attribute1 + ' ' + input_data\n",
    "        return {'pred': prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: mikhailry.\n",
      "View Weave data at https://wandb.ai/mikhailry-paylocity/intro-example/weave\n",
      "游꼴 https://wandb.ai/mikhailry-paylocity/intro-example/r/call/0192cb61-c4f4-73f0-8159-fee1f0cd746e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred': 'hello world'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "weave.init('intro-example')\n",
    "\n",
    "model = RainyLLM(attribute1='hello', attribute2=5)\n",
    "model.predict('world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/mikhailry-paylocity/intro-example/r/call/0192cb7e-72d9-79d0-92d3-803362449a2c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred': 'howdy world'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n",
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n",
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n",
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n",
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n",
      "Task failed: SyntaxError: invalid syntax (<unknown>, line 1)\n"
     ]
    }
   ],
   "source": [
    "weave.init('intro-example')\n",
    "\n",
    "model = RainyLLM(attribute1='howdy', attribute2=10)\n",
    "model.predict('world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS \n",
    "enable you to collect examples for evaluation and automatically track versions for accurate comparisons. Use this to download the latest version locally with a simple API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游닍 Published to https://wandb.ai/mikhailry-paylocity/intro-example/weave/objects/grammar/versions/iaXZ0zJvJ6t4rIcqqavobD2sXIfwP4RcRNWIkYUyMyw\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Dataset\n",
    "\n",
    "# Initialize Weave\n",
    "weave.init('intro-example')\n",
    "\n",
    "# Create a dataset\n",
    "dataset = Dataset(name='grammar', rows=[\n",
    "    {'id': '0', 'sentence': \"He no likes ice cream.\", 'correction': \"He doesn't like ice cream.\"},\n",
    "    {'id': '1', 'sentence': \"She goed to the store.\", 'correction': \"She went to the store.\"},\n",
    "    {'id': '2', 'sentence': \"They plays video games all day.\", 'correction': \"They play video games all day.\"}\n",
    "])\n",
    "\n",
    "# Publish the dataset\n",
    "weave.publish(dataset)\n",
    "\n",
    "# Retrieve the dataset\n",
    "dataset_ref = weave.ref('grammar').get()\n",
    "\n",
    "# Access a specific example\n",
    "example_label = dataset_ref.rows[2]['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The Evaluation class is designed to assess the performance of a Model on a given Dataset or set of examples using scoring functions.\n",
    "To systematically improve your application, it's helpful to test your changes against a consistent dataset of potential inputs so that you catch regressions and can inspect your apps behaviour under different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m3\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_output'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'exact_match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.004947821299235026</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model_output'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'exact_match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.004947821299235026\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/mikhailry-paylocity/intro-example/r/call/0192ce39-faf1-7851-9bf4-972ff3eac5ad\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# Collect your examples\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
    "    {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
    "]\n",
    "\n",
    "# Define any custom scoring function\n",
    "@weave.op()\n",
    "def exact_match(expected: str, model_output: dict) -> dict:\n",
    "    # Here is where you'd define the logic to score the model output\n",
    "    return {\"match\": expected == model_output}\n",
    "\n",
    "\n",
    "# Score your examples using scoring functions\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples,  # can be a list of dictionaries or a weave.Dataset object\n",
    "    scorers=[exact_match],  # can be a list of scoring functions\n",
    "    preprocess_model_input=lambda example: {\"expected\": example[\"expected\"], \"model_output\": example[\"question\"]}\n",
    ")\n",
    "\n",
    "# Start tracking the evaluation\n",
    "weave.init('intro-example')\n",
    "# Run the evaluation\n",
    "summary = await evaluation.evaluate(exact_match)  # can be a model or simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Model to evaluate\n",
    "\n",
    "To evaluate a Model, call evaluate on it using an Evaluation. Models are used when you have attributes that you want to experiment with and capture in weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m Evaluation(\n\u001b[1;32m     15\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mexamples, scorers\u001b[38;5;241m=\u001b[39m[match_score1]\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m weave\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintro-example\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# begin tracking results with weave\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda/anaconda3/envs/LLMs/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from weave import Model, Evaluation\n",
    "import asyncio\n",
    "\n",
    "class MyModel(Model):\n",
    "    prompt: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        # here's where you would add your LLM call and return the output\n",
    "        return {'generated_text': 'Hello, ' + self.prompt}\n",
    "\n",
    "model = MyModel(prompt='World')\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=examples, scorers=[match_score1]\n",
    ")\n",
    "weave.init('intro-example') # begin tracking results with weave\n",
    "asyncio.run(evaluation.evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "allows you to track the inputs and outputs of functions seamlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/mikhailry-paylocity/intro-example/r/call/0192ce76-d748-7012-9907-5897a92f9871\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "@weave.op()\n",
    "def extract_fruit(sentence: str) -> dict:\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be provided with unstructured data, and your task is to parse it one JSON dictionary with fruit, color and flavor as keys.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": sentence\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    extracted = response.choices[0].message.content\n",
    "    return json.loads(extracted)\n",
    "\n",
    "weave.init('intro-example')\n",
    "sentence = \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\"\n",
    "\n",
    "with weave.attributes({'user_id': 'lukas', 'env': 'production'}):\n",
    "    extract_fruit(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
